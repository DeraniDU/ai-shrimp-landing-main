{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost-based Decision Agent\n",
        "\n",
        "Goal: keep a *simple* ML-based decision agent with the same public API used by `ManagerAgent`:\n",
        "- `make_decision(...)`\n",
        "- `make_multi_pond_decisions(...)`\n",
        "\n",
        "## Comparison to AutoGluon\n",
        "- Fewer moving parts (2 models vs 4 predictors)\n",
        "- No pandas dependency in the runtime path\n",
        "- Fast training/inference and easy deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from models import WaterQualityData, FeedData, EnergyData, LaborData\n",
        "from models.decision_model import FeatureExtractor\n",
        "from models.decision_outputs import ActionType, DecisionOutput, MultiPondDecision\n",
        "\n",
        "try:\n",
        "    import joblib\n",
        "    import xgboost as xgb\n",
        "\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except Exception:  # pragma: no cover\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "# OpenAI integration for enhanced explanations\n",
        "try:\n",
        "    from langchain_openai import ChatOpenAI  # type: ignore\n",
        "    OPENAI_AVAILABLE = True\n",
        "except Exception:  # pragma: no cover\n",
        "    try:\n",
        "        from langchain.chat_models import ChatOpenAI  # type: ignore\n",
        "        OPENAI_AVAILABLE = True\n",
        "    except Exception:  # pragma: no cover\n",
        "        OPENAI_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from config import OPENAI_API_KEY, OPENAI_MODEL_NAME, OPENAI_TEMPERATURE\n",
        "except Exception:  # pragma: no cover\n",
        "    OPENAI_API_KEY = None\n",
        "    OPENAI_MODEL_NAME = \"gpt-4o-mini\"\n",
        "    OPENAI_TEMPERATURE = 0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoostDecisionAgent Class\n",
        "\n",
        "Minimal ML decision agent powered by XGBoost.\n",
        "\n",
        "**Models:**\n",
        "- `action_model`: XGBClassifier predicting integer action_type in [0..7]\n",
        "- `urgency_model`: XGBRegressor predicting urgency in [0..1]\n",
        "\n",
        "**Note:** Training may encode observed classes to 0..K-1 and store a mapping in `action_class_mapping.json`. At runtime we map back to the original 0..7 ActionType IDs.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class XGBoostDecisionAgent:\n",
        "    \"\"\"\n",
        "    Minimal ML decision agent powered by XGBoost.\n",
        "\n",
        "    Models:\n",
        "    - action_model: XGBClassifier predicting integer action_type in [0..7]\n",
        "    - urgency_model: XGBRegressor predicting urgency in [0..1]\n",
        "\n",
        "    Note: training may encode observed classes to 0..K-1 and store a mapping\n",
        "    in `action_class_mapping.json`. At runtime we map back to the original\n",
        "    0..7 ActionType IDs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dir: str = \"models/xgboost_models\", enable_llm_explanations: bool = True):\n",
        "        if not XGBOOST_AVAILABLE:\n",
        "            raise ImportError(\"XGBoost dependencies not installed. Install with: pip install xgboost joblib\")\n",
        "\n",
        "        self.model_dir = Path(model_dir)\n",
        "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "        self.action_model: Optional[\"xgb.XGBClassifier\"] = None\n",
        "        self.urgency_model: Optional[\"xgb.XGBRegressor\"] = None\n",
        "        self._enc_to_orig: Optional[Dict[int, int]] = None\n",
        "\n",
        "        # OpenAI LLM for enhanced explanations (optional)\n",
        "        self.llm: Optional[ChatOpenAI] = None\n",
        "        self.enable_llm_explanations = enable_llm_explanations and OPENAI_AVAILABLE\n",
        "        if self.enable_llm_explanations and OPENAI_API_KEY:\n",
        "            try:\n",
        "                self.llm = ChatOpenAI(\n",
        "                    openai_api_key=OPENAI_API_KEY,\n",
        "                    model_name=OPENAI_MODEL_NAME,\n",
        "                    temperature=OPENAI_TEMPERATURE,\n",
        "                )\n",
        "            except Exception:\n",
        "                self.enable_llm_explanations = False\n",
        "                self.llm = None\n",
        "\n",
        "        # For compatibility with code that checks this flag.\n",
        "        self.is_trained = False\n",
        "        self._try_load_models()\n",
        "\n",
        "    def _try_load_models(self) -> None:\n",
        "        action_path = self.model_dir / \"action_model.pkl\"\n",
        "        urgency_path = self.model_dir / \"urgency_model.pkl\"\n",
        "        mapping_path = self.model_dir / \"action_class_mapping.json\"\n",
        "\n",
        "        if action_path.exists() and urgency_path.exists():\n",
        "            self.action_model = joblib.load(action_path)\n",
        "            self.urgency_model = joblib.load(urgency_path)\n",
        "\n",
        "            if mapping_path.exists():\n",
        "                try:\n",
        "                    payload = json.loads(mapping_path.read_text(encoding=\"utf-8\"))\n",
        "                    enc_to_orig = payload.get(\"enc_to_orig\") or {}\n",
        "                    # JSON keys may be strings; normalize to int->int.\n",
        "                    self._enc_to_orig = {int(k): int(v) for k, v in enc_to_orig.items()}\n",
        "                except Exception:\n",
        "                    self._enc_to_orig = None\n",
        "\n",
        "            self.is_trained = True\n",
        "\n",
        "    def make_decision(  \n",
        "        self,\n",
        "        water_quality_data: List[WaterQualityData],\n",
        "        feed_data: List[FeedData],\n",
        "        energy_data: List[EnergyData],\n",
        "        labor_data: List[LaborData],\n",
        "        pond_id: Optional[int] = None,\n",
        "    ) -> DecisionOutput:\n",
        "        if not self.is_trained or self.action_model is None or self.urgency_model is None:\n",
        "            raise ValueError(\"XGBoost models not trained/loaded. Train with: python train_xgboost_models.py\")\n",
        "\n",
        "        if not water_quality_data or not feed_data or not energy_data or not labor_data:\n",
        "            raise ValueError(\"All data lists must be non-empty\")\n",
        "\n",
        "        if pond_id is None:\n",
        "            pond_id = water_quality_data[0].pond_id\n",
        "\n",
        "        wq = next((w for w in water_quality_data if w.pond_id == pond_id), water_quality_data[0])\n",
        "        feed = next((f for f in feed_data if f.pond_id == pond_id), feed_data[0])\n",
        "        energy = next((e for e in energy_data if e.pond_id == pond_id), energy_data[0])\n",
        "        labor = next((l for l in labor_data if l.pond_id == pond_id), labor_data[0])\n",
        "\n",
        "        # FeatureExtractor returns a 35-float vector for a single pond.\n",
        "        x = self.feature_extractor.extract_features([wq], [feed], [energy], [labor])\n",
        "        x = np.asarray(x, dtype=np.float32).reshape(1, -1)\n",
        "\n",
        "        action_pred = int(self.action_model.predict(x)[0])\n",
        "        action_idx = int(self._enc_to_orig.get(action_pred, action_pred) if self._enc_to_orig else action_pred)\n",
        "        action_idx = max(0, min(7, action_idx))\n",
        "\n",
        "        urgency = float(self.urgency_model.predict(x)[0])\n",
        "        urgency = float(max(0.0, min(1.0, urgency)))\n",
        "\n",
        "        confidence = self._confidence(x, urgency)\n",
        "        primary_action = self._map_action(action_idx)\n",
        "\n",
        "        # Generate enhanced reasoning with OpenAI if available\n",
        "        reasoning = self._generate_reasoning(\n",
        "            primary_action=primary_action,\n",
        "            urgency=urgency,\n",
        "            confidence=confidence,\n",
        "            wq=wq,\n",
        "            feed=feed,\n",
        "            energy=energy,\n",
        "            labor=labor,\n",
        "        )\n",
        "\n",
        "        return DecisionOutput(\n",
        "            timestamp=datetime.now(),\n",
        "            pond_id=int(pond_id),\n",
        "            primary_action=primary_action,\n",
        "            action_intensity=urgency,\n",
        "            secondary_actions=[],\n",
        "            priority_rank=1,  # overwritten in multi-pond pass\n",
        "            urgency_score=urgency,\n",
        "            # Keep optional optimization outputs unset for maximum simplicity.\n",
        "            recommended_feed_amount=None,\n",
        "            recommended_aerator_level=None,\n",
        "            recommended_pump_level=None,\n",
        "            recommended_heater_level=None,\n",
        "            confidence=confidence,\n",
        "            reasoning=reasoning,\n",
        "            affected_factors=self._affected_factors(wq, energy, labor),\n",
        "        )\n",
        "\n",
        "    def make_multi_pond_decisions(\n",
        "        self,\n",
        "        water_quality_data: List[WaterQualityData],\n",
        "        feed_data: List[FeedData],\n",
        "        energy_data: List[EnergyData],\n",
        "        labor_data: List[LaborData],\n",
        "    ) -> MultiPondDecision:\n",
        "        decisions: Dict[int, DecisionOutput] = {}\n",
        "\n",
        "        pond_ids = [wq.pond_id for wq in water_quality_data]\n",
        "        for pid in pond_ids:\n",
        "            decisions[pid] = self.make_decision(\n",
        "                water_quality_data=water_quality_data,\n",
        "                feed_data=feed_data,\n",
        "                energy_data=energy_data,\n",
        "                labor_data=labor_data,\n",
        "                pond_id=pid,\n",
        "            )\n",
        "\n",
        "        # Assign priority ranks: 1 = most urgent\n",
        "        urgency_sorted = sorted(decisions.items(), key=lambda kv: kv[1].urgency_score, reverse=True)\n",
        "        pond_priorities: Dict[int, int] = {}\n",
        "        for rank, (pid, decision) in enumerate(urgency_sorted, start=1):\n",
        "            # Be robust across pydantic v1/v2: avoid relying on in-place mutation.\n",
        "            try:\n",
        "                decisions[pid] = decision.model_copy(update={\"priority_rank\": rank})  # pydantic v2\n",
        "            except Exception:\n",
        "                try:\n",
        "                    decisions[pid] = decision.copy(update={\"priority_rank\": rank})  # pydantic v1\n",
        "                except Exception:\n",
        "                    decision.priority_rank = rank  # last resort\n",
        "                    decisions[pid] = decision\n",
        "            pond_priorities[pid] = rank\n",
        "\n",
        "        urgent_ponds = [pid for pid, d in decisions.items() if d.urgency_score >= 0.7]\n",
        "        overall_urgency = max((d.urgency_score for d in decisions.values()), default=0.0)\n",
        "\n",
        "        total_urgency = sum(d.urgency_score for d in decisions.values())\n",
        "        resource_allocation: Dict[str, float] = {}\n",
        "        if total_urgency > 0:\n",
        "            for pid, d in decisions.items():\n",
        "                resource_allocation[f\"pond_{pid}\"] = d.urgency_score / total_urgency\n",
        "\n",
        "        return MultiPondDecision(\n",
        "            timestamp=datetime.now(),\n",
        "            pond_priorities=pond_priorities,\n",
        "            urgent_ponds=urgent_ponds,\n",
        "            recommended_actions=decisions,\n",
        "            overall_urgency=overall_urgency,\n",
        "            resource_allocation=resource_allocation,\n",
        "        )\n",
        "\n",
        "    def _confidence(self, x: np.ndarray, urgency: float) -> float:\n",
        "        try:\n",
        "            proba = self.action_model.predict_proba(x)  # type: ignore[union-attr]\n",
        "            return float(np.max(proba))\n",
        "        except Exception:\n",
        "            # Fallback: scale modestly with urgency (mirrors rule-based agents).\n",
        "            return float(max(0.6, min(0.85, 0.6 + urgency * 0.25)))\n",
        "\n",
        "    @staticmethod\n",
        "    def _map_action(action_idx: int) -> ActionType:\n",
        "        action_type_map = {\n",
        "            0: ActionType.NO_ACTION,\n",
        "            1: ActionType.INCREASE_AERATION,\n",
        "            2: ActionType.DECREASE_AERATION,\n",
        "            3: ActionType.WATER_EXCHANGE,\n",
        "            4: ActionType.ADJUST_FEED,\n",
        "            5: ActionType.EMERGENCY_RESPONSE,\n",
        "            6: ActionType.ALLOCATE_WORKERS,\n",
        "            7: ActionType.MONITOR_CLOSELY,\n",
        "        }\n",
        "        return action_type_map.get(int(action_idx), ActionType.NO_ACTION)\n",
        "\n",
        "    def _generate_reasoning(\n",
        "        self,\n",
        "        primary_action: ActionType,\n",
        "        urgency: float,\n",
        "        confidence: float,\n",
        "        wq: WaterQualityData,\n",
        "        feed: FeedData,\n",
        "        energy: EnergyData,\n",
        "        labor: LaborData,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate detailed reasoning explanation for the decision.\n",
        "        Uses OpenAI LLM if available, otherwise falls back to template-based explanation.\n",
        "        \"\"\"\n",
        "        # Fallback to simple explanation if LLM is not available\n",
        "        if not self.enable_llm_explanations or not self.llm:\n",
        "            return self._generate_fallback_reasoning(\n",
        "                primary_action, urgency, confidence, wq, feed, energy, labor\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            # Build context for LLM\n",
        "            context = self._build_decision_context(wq, feed, energy, labor)\n",
        "            \n",
        "            prompt = f\"\"\"You are an expert aquaculture specialist analyzing shrimp farm operations. \n",
        "Based on the following data, provide a clear, actionable explanation for why the AI system recommended \n",
        "the action \"{primary_action.value}\" with urgency {urgency:.2f} and confidence {confidence:.2f}.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Provide a concise but comprehensive explanation (2-3 sentences) that:\n",
        "1. Explains the key factors that led to this decision\n",
        "2. Describes why this action is appropriate given the current conditions\n",
        "3. Mentions any specific parameters that triggered the recommendation\n",
        "\n",
        "Be professional, clear, and actionable. Focus on the most critical issues.\"\"\"\n",
        "\n",
        "            # Use invoke with a simple string message (compatible with both old and new langchain)\n",
        "            try:\n",
        "                # Try new langchain API first\n",
        "                from langchain_core.messages import HumanMessage  # type: ignore\n",
        "                response = self.llm.invoke([HumanMessage(content=prompt)])\n",
        "            except Exception:\n",
        "                # Fallback to older API\n",
        "                try:\n",
        "                    from langchain.schema import HumanMessage  # type: ignore\n",
        "                    response = self.llm.invoke([HumanMessage(content=prompt)])\n",
        "                except Exception:\n",
        "                    # Direct call as last resort\n",
        "                    response = self.llm.invoke(prompt)\n",
        "            \n",
        "            # Extract content from response\n",
        "            if hasattr(response, 'content'):\n",
        "                explanation = response.content.strip()\n",
        "            elif isinstance(response, str):\n",
        "                explanation = response.strip()\n",
        "            else:\n",
        "                explanation = str(response).strip()\n",
        "            \n",
        "            # Ensure we have a valid explanation\n",
        "            if explanation and len(explanation) > 20:\n",
        "                return explanation\n",
        "            else:\n",
        "                return self._generate_fallback_reasoning(\n",
        "                    primary_action, urgency, confidence, wq, feed, energy, labor\n",
        "                )\n",
        "        except Exception as e:\n",
        "            # Fallback on any error\n",
        "            return self._generate_fallback_reasoning(\n",
        "                primary_action, urgency, confidence, wq, feed, energy, labor\n",
        "            )\n",
        "\n",
        "    def _build_decision_context(\n",
        "        self,\n",
        "        wq: WaterQualityData,\n",
        "        feed: FeedData,\n",
        "        energy: EnergyData,\n",
        "        labor: LaborData,\n",
        "    ) -> str:\n",
        "        \"\"\"Build a structured context string for LLM reasoning.\"\"\"\n",
        "        context_parts = [\n",
        "            f\"Water Quality Status: {wq.status.value}\",\n",
        "            f\"Dissolved Oxygen: {wq.dissolved_oxygen:.2f} mg/L (optimal: >5.0)\",\n",
        "            f\"Ammonia: {wq.ammonia:.3f} mg/L (optimal: <0.2)\",\n",
        "            f\"Nitrite: {wq.nitrite:.3f} mg/L\",\n",
        "            f\"pH: {wq.ph:.2f} (optimal: 7.5-8.5)\",\n",
        "            f\"Temperature: {wq.temperature:.1f}°C (optimal: 26-30°C)\",\n",
        "            f\"Salinity: {wq.salinity:.1f} ppt (optimal: 15-25)\",\n",
        "        ]\n",
        "        \n",
        "        if feed:\n",
        "            context_parts.append(\n",
        "                f\"Feed: {feed.feed_amount:.1f}g per serving, \"\n",
        "                f\"Average weight: {feed.average_weight:.1f}g, \"\n",
        "                f\"Shrimp count: {feed.shrimp_count}\"\n",
        "            )\n",
        "        \n",
        "        if energy:\n",
        "            context_parts.append(\n",
        "                f\"Energy Efficiency: {energy.efficiency_score:.2f}, \"\n",
        "                f\"Aerator usage: {energy.aerator_usage:.1%}, \"\n",
        "                f\"Pump usage: {energy.pump_usage:.1%}\"\n",
        "            )\n",
        "        \n",
        "        if labor:\n",
        "            context_parts.append(\n",
        "                f\"Labor Efficiency: {labor.efficiency_score:.2f}, \"\n",
        "                f\"Workers: {labor.worker_count}, \"\n",
        "                f\"Pending tasks: {len(labor.next_tasks)}\"\n",
        "            )\n",
        "        \n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_fallback_reasoning(\n",
        "        primary_action: ActionType,\n",
        "        urgency: float,\n",
        "        confidence: float,\n",
        "        wq: WaterQualityData,\n",
        "        feed: FeedData,\n",
        "        energy: EnergyData,\n",
        "        labor: LaborData,\n",
        "    ) -> str:\n",
        "        \"\"\"Generate a template-based reasoning when LLM is not available.\"\"\"\n",
        "        base_reasoning = f\"XGBoost model recommended {primary_action.value} with urgency {urgency:.2f} and confidence {confidence:.2f}.\"\n",
        "        \n",
        "        # Add context-specific details\n",
        "        details = []\n",
        "        \n",
        "        if primary_action == ActionType.EMERGENCY_RESPONSE:\n",
        "            if wq.dissolved_oxygen < 5.0:\n",
        "                details.append(f\"Critical: Dissolved oxygen is dangerously low at {wq.dissolved_oxygen:.2f} mg/L.\")\n",
        "            if wq.ammonia > 0.2:\n",
        "                details.append(f\"Critical: Ammonia levels are elevated at {wq.ammonia:.3f} mg/L.\")\n",
        "            details.append(\"Immediate intervention required to prevent shrimp mortality.\")\n",
        "        \n",
        "        elif primary_action == ActionType.INCREASE_AERATION:\n",
        "            details.append(f\"Dissolved oxygen is {wq.dissolved_oxygen:.2f} mg/L, below optimal levels (>5.0 mg/L).\")\n",
        "            details.append(\"Increasing aeration will improve oxygen saturation and shrimp health.\")\n",
        "        \n",
        "        elif primary_action == ActionType.WATER_EXCHANGE:\n",
        "            if wq.ammonia > 0.2:\n",
        "                details.append(f\"Ammonia levels at {wq.ammonia:.3f} mg/L exceed safe thresholds.\")\n",
        "            if wq.nitrite > 0.1:\n",
        "                details.append(f\"Nitrite levels at {wq.nitrite:.3f} mg/L are elevated.\")\n",
        "            details.append(\"Partial water exchange will dilute toxins and improve water quality.\")\n",
        "        \n",
        "        elif primary_action == ActionType.ADJUST_FEED:\n",
        "            if wq.dissolved_oxygen < 5.0 or wq.ammonia > 0.2:\n",
        "                details.append(\"Water quality stress detected. Reducing feed will decrease metabolic load.\")\n",
        "            else:\n",
        "                details.append(f\"Current feed: {feed.feed_amount:.1f}g. Adjusting based on growth stage and conditions.\")\n",
        "        \n",
        "        elif primary_action == ActionType.ALLOCATE_WORKERS:\n",
        "            details.append(f\"Labor efficiency at {labor.efficiency_score:.2f} with {len(labor.next_tasks)} pending tasks.\")\n",
        "            details.append(\"Reallocating workers to prioritize critical operations.\")\n",
        "        \n",
        "        elif primary_action == ActionType.MONITOR_CLOSELY:\n",
        "            details.append(f\"Water quality status: {wq.status.value}. Conditions require close monitoring.\")\n",
        "            details.append(\"No immediate action needed, but parameters should be tracked for trends.\")\n",
        "        \n",
        "        if details:\n",
        "            return base_reasoning + \" \" + \" \".join(details)\n",
        "        return base_reasoning\n",
        "\n",
        "    @staticmethod\n",
        "    def _affected_factors(wq: WaterQualityData, energy: EnergyData, labor: LaborData) -> List[str]:\n",
        "        factors: List[str] = []\n",
        "        if wq.status.value in [\"poor\", \"critical\"]:\n",
        "            factors.append(\"Water Quality\")\n",
        "        if wq.dissolved_oxygen < 5.0:\n",
        "            factors.append(\"Dissolved Oxygen\")\n",
        "        if wq.ammonia > 0.2:\n",
        "            factors.append(\"Ammonia Levels\")\n",
        "        if energy.efficiency_score < 0.7:\n",
        "            factors.append(\"Energy Efficiency\")\n",
        "        if labor.efficiency_score < 0.7:\n",
        "            factors.append(\"Labor Efficiency\")\n",
        "        return factors if factors else [\"Normal Operations\"]\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
